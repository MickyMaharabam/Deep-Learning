{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+DROhcknWFIEuybT+5CPD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"el__fWekxnJQ","executionInfo":{"status":"ok","timestamp":1751219522181,"user_tz":-330,"elapsed":53813,"user":{"displayName":"Maharabam Micky Devi","userId":"03527759001385913701"}},"outputId":"12951c82-b586-4adc-f3dd-6566d5914d82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Network architecture: [784, 256, 128, 64, 32, 10] (4 hidden layers)\n","Sample weights (first layer, first 5 values): [0.0890981  0.02021099 0.04943373 0.1131822  0.09432592]\n","Starting training...\n","Starting epoch 0\n","Epoch 0, Loss: 0.4567, Train Accuracy: 0.8707\n","Starting epoch 1\n","Epoch 1, Loss: 0.3238, Train Accuracy: 0.9062\n","Starting epoch 2\n","Epoch 2, Loss: 0.2745, Train Accuracy: 0.9197\n","Starting epoch 3\n","Epoch 3, Loss: 0.2440, Train Accuracy: 0.9288\n","Starting epoch 4\n","Epoch 4, Loss: 0.2215, Train Accuracy: 0.9355\n","Starting epoch 5\n","Epoch 5, Loss: 0.2036, Train Accuracy: 0.9404\n","Starting epoch 6\n","Epoch 6, Loss: 0.1882, Train Accuracy: 0.9447\n","Starting epoch 7\n","Epoch 7, Loss: 0.1743, Train Accuracy: 0.9489\n","Starting epoch 8\n","Epoch 8, Loss: 0.1626, Train Accuracy: 0.9524\n","Starting epoch 9\n","Epoch 9, Loss: 0.1524, Train Accuracy: 0.9554\n","Test Accuracy: 0.9507\n"]}],"source":["import numpy as np\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","\n","# Set random seed for reproducibility\n","np.random.seed(0)\n","\n","# Activation functions\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_derivative(x):\n","    return np.where(x > 0, 1, 0)\n","\n","def softmax(x):\n","    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n","    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n","\n","# Loss function: Categorical cross-entropy\n","def categorical_cross_entropy(y_true, y_pred):\n","    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n","    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[1]\n","\n","# Derivative of categorical cross-entropy w.r.t. softmax output\n","def cce_derivative(y_true, y_pred):\n","    return y_pred - y_true\n","\n","# Neural Network class\n","class NeuralNetwork:\n","    def __init__(self, layer_sizes):\n","        self.layer_sizes = layer_sizes\n","        self.weights = []\n","        self.biases = []\n","        self.initialize_parameters()\n","        print(f\"Network architecture: {self.layer_sizes} (4 hidden layers)\")\n","        print(f\"Sample weights (first layer, first 5 values): {self.weights[0].flatten()[:5]}\")\n","\n","    def initialize_parameters(self):\n","        for i in range(1, len(self.layer_sizes)):\n","            # He initialization for ReLU layers\n","            self.weights.append(np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1]) * np.sqrt(2.0 / self.layer_sizes[i-1]))\n","            self.biases.append(np.zeros((self.layer_sizes[i], 1)))\n","\n","    def forward_propagation(self, X):\n","        self.A = [X.T]\n","        self.Z = []\n","\n","        # Hidden layers (ReLU)\n","        for i in range(len(self.weights) - 1):\n","            Z = np.dot(self.weights[i], self.A[-1]) + self.biases[i]\n","            self.Z.append(Z)\n","            self.A.append(relu(Z))\n","\n","        # Output layer (Softmax)\n","        Z = np.dot(self.weights[-1], self.A[-1]) + self.biases[-1]\n","        self.Z.append(Z)\n","        self.A.append(softmax(Z))\n","\n","        return self.A[-1].T\n","\n","    def backpropagation(self, X, y, y_pred, learning_rate):\n","        m = X.shape[0]\n","        y = y.T\n","\n","        dA = cce_derivative(y, y_pred.T)\n","\n","        dW = []\n","        db = []\n","        for i in range(len(self.weights) - 1, -1, -1):\n","            if i == len(self.weights) - 1:\n","                dZ = dA\n","            else:\n","                dZ = dA * relu_derivative(self.Z[i])\n","\n","            dW.insert(0, np.dot(dZ, self.A[i].T) / m)\n","            db.insert(0, np.sum(dZ, axis=1, keepdims=True) / m)\n","            if i > 0:\n","                dA = np.dot(self.weights[i].T, dZ)\n","\n","        for i in range(len(self.weights)):\n","            self.weights[i] -= learning_rate * dW[i]\n","            self.biases[i] -= learning_rate * db[i]\n","\n","    def predict(self, X):\n","        y_pred = self.forward_propagation(X)\n","        return np.argmax(y_pred, axis=1)\n","\n","    def train(self, X, y, epochs, learning_rate, batch_size=128):\n","        m = X.shape[0]\n","        print(\"Starting training...\")\n","        try:\n","            for epoch in range(epochs):\n","                print(f\"Starting epoch {epoch}\")\n","                # Mini-batch gradient descent\n","                for i in range(0, m, batch_size):\n","                    X_batch = X[i:i+batch_size]\n","                    y_batch = y[i:i+batch_size]\n","                    y_pred = self.forward_propagation(X_batch)\n","                    self.backpropagation(X_batch, y_batch, y_pred, learning_rate)\n","\n","                # Compute loss and training accuracy\n","                y_pred = self.forward_propagation(X)\n","                loss = categorical_cross_entropy(y.T, y_pred.T)\n","                train_accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n","                print(f\"Epoch {epoch}, Loss: {loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n","        except Exception as e:\n","            print(f\"Training interrupted: {e}\")\n","\n","# Load and preprocess MNIST dataset\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","# Flatten and normalize\n","X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n","X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n","\n","# One-hot encode labels\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","\n","# Define network: 784 input neurons, 4 hidden layers (256, 128, 64, 32), 10 output neurons\n","nn = NeuralNetwork([784, 256, 128, 64, 32, 10])\n","\n","# Train the network for 10 epochs\n","nn.train(X_train, y_train, epochs=10, learning_rate=0.01, batch_size=128)\n","\n","# Evaluate on test set\n","y_pred = nn.predict(X_test)\n","test_accuracy = np.mean(y_pred == np.argmax(y_test, axis=1))\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]}]}